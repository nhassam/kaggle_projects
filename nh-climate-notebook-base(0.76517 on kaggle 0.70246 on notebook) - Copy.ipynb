{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom imblearn.over_sampling import SMOTE","execution_count":210,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bring in Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/climate-change-edsa2020-21/train.csv')\ntest = pd.read_csv('../input/climate-change-edsa2020-21/test.csv')","execution_count":211,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## Remove urls\nprint ('Removing URLs...')\npattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\n#train['message'] = train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n#test['message'] = test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n\n# Make lower case\nprint ('Lowering case...')\n#train['message'] = train['message'].str.lower()\n#test['message'] = test['message'].str.lower()\n\n# Remove punctuation\nimport string\n#print ('Cleaning punctuation...')\ndef remove_punctuation_numbers(post):\n    punc_numbers = string.punctuation + '0123456789'\n    return ''.join([l for l in post if l not in punc_numbers])\n#train['message'] = train['message'].apply(remove_punctuation_numbers)\n#test['message'] = test['message'].apply(remove_punctuation_numbers)\n\n","execution_count":212,"outputs":[{"output_type":"stream","text":"Removing URLs...\nLowering case...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialise common text & social media abbreviations as dictionary\ntext_abb = {\"aka\": \"also known as\", \"btw\": \"by the way\",\n            \"b/c\": \"because\", \"fyi\": \"for your information\",\n            \"idk\": \"i do not know\", \"lol\": \"laughing out loud\",\n            \"lmao\": \"laughing my ass off\", \"lmfao\": \"laughing\",\n            \"omg\": \"oh my god\", \"otoh\": \"on the other hand\",\n            \"wth\": \"what the hell\", \"wtf\": \"what the fuck\",\n            \"icymi\": \"in case you missed it\",\n            \"rofl\": \"rolling on the floor laughing\",\n            \"stfu\": \"shut the fuck up\", \"nvm\": \"nevermind\",\n            \"luv\": \"love\", \"luvs\": \"loves\",\"qanda\":\"question and answer\",\n            \"maga\": \"make america great again\"\n            }\n# update text abbreviations\ntext_abb['nodapl'] = 'no dakota access pipeline'","execution_count":213,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create stop words list\nall_stopwords = stopwords.words('english')\n# update stop words list for specific project\nall_stopwords.extend(['amp']) \nall_stopwords.remove('not')","execution_count":214,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialise common contractions as dictionary\ncontractions = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n                \"can't've\": \"cannot have\", \"'cause\": \"because\",\n                \"could've\": \"could have\", \"couldn't\": \"could not\",\n                \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n                \"doesn't\": \"does not\", \"don't\": \"do not\",\n                \"everyone's\": \"everyone is\", \"finna\": \"going to\",\n                \"gimme\": \"give me\", \"gonna\": \"going to\", \"gotta\": \"got to\",\n                \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n                \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n                \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n                \"he'll've\": \"he he will have\", \"he's\": \"he is\",\n                \"how'd\": \"how did\", \"howdy\": \"how do you do\",\n                \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\",\n                \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n                \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n                \"i'm'a\": \"I am about to\", \"i've\": \"i have\",\n                \"innit\": \"is it not\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                \"mayn't\": \"may not\", \"might've\": \"might have\",\n                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n                \"must've\": \"must have\", \"mustn't\": \"must not\",\n                \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                \"needn't've\": \"need not have\",\n                \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n                \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n                \"she's\": \"she is\", \"should've\": \"should have\",\n                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n                \"so've\": \"so have\", \"so's\": \"so as\", \"that'd\": \"that would\",\n                \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n                \"there's\": \"there is\", \"they'd\": \"they would\",\n                \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                \"they'll've\": \"they will have\", \"they're\": \"they are\",\n                \"they've\": \"they have\", \"to've\": \"to have\",\n                \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n                \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                \"we've\": \"we have\", \"weren't\": \"were not\",\n                \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n                \"what're\": \"what are\", \"what's\": \"what is\",\n                \"what've\": \"what have\", \"when's\": \"when is\",\n                \"when've\": \"when have\", \"where'd\": \"where did\",\n                \"where's\": \"where is\", \"where've\": \"where have\",\n                \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n                \"why've\": \"why have\", \"will've\": \"will have\",\n                \"won't\": \"will not\", \"won't've\": \"will not have\",\n                \"would've\": \"would have\", \"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\",\n                \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\",\n                \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n                \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                \"you'll've\": \"you will have\", \"you're\": \"you are\",\n                \"you've\": \"you have\"\n                }","execution_count":215,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tweet_preprocessing(df, cont_dict, abbr_dict, all_stopwords):\n    \"\"\"\n    This function conducts preprocessing of tweet data for a\n    machine learning model including lowercasing, expansion of\n    contractions & common text abbrieviations, and removal of\n    punctuation and stopwords and stemming.\n    \n    Arguments:\n    df: original dataframe column containing tweets\n    cont_dict: dictionary of contractions to be expanded\n    abbr_dict: dictionary of text abbreviations to be expanded\n    all_stopwords: list of stopwords to be removed\n    \n    Returns:\n    modified dataframe of cleaned tweets\n    \"\"\"\n    # convert to lower case\n    clean_df = df.str.lower()\n    # expand contractions & abbreviations\n    clean_df = clean_df.replace(contractions, regex=True)\n    clean_df = clean_df.replace(text_abb, regex=True)\n    # replace urls\n    pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    clean_df = clean_df.str.replace(pattern_url, \"weburl\", regex=True)\n    # remove punctuation & noise\n    clean_df = clean_df.str.replace(\"[^a-z@#!?]\", \" \")\n    clean_df = clean_df.apply(lambda x: ' '.join([word for word in x.split()\n                                                  if len(word) > 2]))\n    # remove stop words\n    clean_df = clean_df.apply(lambda x: ' '.join\n                             ([word for word in x.split()\n                               if word not in set(all_stopwords)]))\n    # reduce word to root form using lemmatization\n    tweet_corpus = []\n    for i in range(0, len(df)):\n        tweet = clean_df[i]\n        lemmatizer = WordNetLemmatizer()\n        tweet = [lemmatizer.lemmatize(word) for word in tweet.split()]\n        tweet = ' '.join(tweet)\n        tweet_corpus.append(tweet)\n    return clean_df","execution_count":216,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process the train and test tweets\n#train['message'] = tweet_preprocessing(train['message'], contractions,text_abb, all_stopwords)\n#test['message'] = tweet_preprocessing(test['message'], contractions,text_abb, all_stopwords)","execution_count":217,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balance the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the  object with the desired sampling strategy.\n#smote = SMOTE(sampling_strategy='minority')\n\n# fit object to training data\n#X_train_smote, y_train_smote = smote.fit_sample(X, y)\n#X_train_smote.shape","execution_count":218,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n#Separate minority and majority classes\nbelieve = train[train['sentiment'] == 1]\nno_belief = train[train['sentiment'] == -1]\nneutral = train[train['sentiment'] == 0]\nnews = train[train['sentiment'] == 2]\n\n#Upsample minority class\nno_belief_upsampled= resample(no_belief,replace= True,\n                            n_samples= len(believe), random_state =0) #sample with replacement\nneutral_upsampled= resample(neutral,replace= True,\n                            n_samples= len(believe), random_state =0) #sample with replacement\nnews_upsampled= resample(news,replace= True,\n                            n_samples= len(believe), random_state =0) #sample with replacement\n\n\n#Combine majority class with upsampled minority class\ndf_upsampled = pd.concat ([believe,no_belief_upsampled,neutral_upsampled,news_upsampled])\n#Display new class counts\ndf_upsampled.sentiment.value_counts()","execution_count":219,"outputs":[{"output_type":"execute_result","execution_count":219,"data":{"text/plain":"-1    8530\n 2    8530\n 1    8530\n 0    8530\nName: sentiment, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If using resampling\n#y = df_upsampled['sentiment']\n#X = df_upsampled['message']\n\n# No resampling\ny = train['sentiment']\nX = train['message']","execution_count":220,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorize the data that model requires"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, \n                      ngram_range=(1, 2), use_idf=2, smooth_idf=2, analyzer='word',\n                      sublinear_tf = True)\nX_vectorized = vectorizer.fit_transform(X)","execution_count":221,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vectorized.shape","execution_count":222,"outputs":[{"output_type":"execute_result","execution_count":222,"data":{"text/plain":"(15819, 33271)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the training data into a training and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X_vectorized,y,test_size=0.1, stratify=y, shuffle=True,random_state=0)\n","execution_count":223,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear SVC model \n#lin_svc = LinearSVC()\nlin_svc = LinearSVC(C=0.7)\nlin_svc.fit(X_train, y_train)\nlin_svc_pred_train = lin_svc.predict(X_train)   \nlin_svc_pred_test = lin_svc.predict(X_test)","execution_count":224,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation and performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear SVC\nprint(f1_score(y_train, lin_svc_pred_train, average=\"macro\"))\nprint(f1_score(y_test, lin_svc_pred_test, average=\"macro\"))","execution_count":225,"outputs":[{"output_type":"stream","text":"0.9807518218398193\n0.7024633578008339\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear SVC\nprint(metrics.confusion_matrix(y_train, lin_svc_pred_train))\nprint(metrics.classification_report(y_test, lin_svc_pred_test))","execution_count":226,"outputs":[{"output_type":"stream","text":"[[1131    4   27    4]\n [   1 2010   88   19]\n [   0    9 7619   49]\n [   0    0   46 3230]]\n              precision    recall  f1-score   support\n\n          -1       0.78      0.52      0.63       130\n           0       0.69      0.44      0.54       235\n           1       0.79      0.90      0.84       853\n           2       0.79      0.81      0.80       364\n\n    accuracy                           0.78      1582\n   macro avg       0.76      0.67      0.70      1582\nweighted avg       0.78      0.78      0.77      1582\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Prepare for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"testx = test['message']\ntest_vect = vectorizer.transform(testx)","execution_count":227,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_actual = lin_svc.predict(test_vect)","execution_count":228,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'tweetid': test['tweetid'],'sentiment': y_pred_actual})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":229,"outputs":[{"output_type":"execute_result","execution_count":229,"data":{"text/plain":"   tweetid  sentiment\n0   169760          1\n1    35326          1\n2   224985          1\n3   476263          1\n4   872928          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweetid</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>169760</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35326</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>224985</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>476263</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>872928</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}